{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents <a id='top'></a>\n",
    "\n",
    "1. <a href=#exploration>CDNOW Dataset</a>\n",
    "    1. <a href=#exploration>Exploration</a>\n",
    "    1. <a href=#rfm>RFM Modeling<a>\n",
    "    1. <a href=#hclust>Hierarchical Clustering</a>\n",
    "1. <a href=#mba>Market Basket Analysis</a>\n",
    "1. <a href=#ref>References and Summary</a>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='exploration'></a>\n",
    "## CDNOW Dataset \n",
    "\n",
    "### Exploration\n",
    "<a href=#top>(back to top)</a>\n",
    "\n",
    "CDNOW is a company that sold CDs online. It was started in 1994, but it closed after the dot-com bust of the late 90s. The dataset that we have comes from the transactions made by a **single cohort** of customers at this online store. The single cohort of customers is defined by customers who made their first purchase within the first 12 weeks of 1997. These customers are then tracked until June 30th 1998. Each row in our dataset consists of the customer id, week of purchase, quantity of CDs purchased and the amount spent in that week. There are 23,570 customers in this cohort and 64,258 transactions in this dataset. Let's read in the dataset first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'myscripts'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrandom\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m default_rng\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcluster\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m hierarchy\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmyscripts\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clust\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      9\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatplotlib\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minline\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'myscripts'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.random import default_rng\n",
    "from scipy.cluster import hierarchy\n",
    "\n",
    "from myscripts import clust\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdnow2 = pd.read_csv('../data/CDNOW2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdnow2.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdnow2[['qty', 'amnt']].describe(percentiles=[0.8, 0.9, 0.99]).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdnow2[['qty', 'amnt']].hist(figsize=(12,4), grid=False, bins=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The median amount spent in a weekly transaction is approximately 27 dollars. This is a very right-skewed distribution. The quantity of CDs bought has a median of 2 per week, but this is also an extremely right-skewed distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='rfm'></a>\n",
    "### RFM Modeling\n",
    "<a href=#top>(back to top)</a>\n",
    "\n",
    "RFM tells you how customers behaved in the past:\n",
    "  * **Recency**: When was the last time they bought something?\n",
    "  * **Frequency**: How frequently do they buy things?\n",
    "  * **Monetary**: How much have they spent?\n",
    "  \n",
    "The ideas behind it are these assumptions:\n",
    "1. Recent customers are more likely to buy than customers who haven't purchased in a while.\n",
    "2. Customers who purchase often are more likely to buy than those who don't purchased often.\n",
    "3. Customers who spend more are more likely to buy again than those who spend less.\n",
    "\n",
    "The purpose of RFM modeling is to yield insights pertaining to:\n",
    "* the best/most valuable customers.\n",
    "* the right time to \"nudge\" or \"remind\" a customer via an offer.\n",
    "* identifying customers for whom it might be fruitful to try an upsell strategy.\n",
    "\n",
    "Here's how RFM works: For each customer, we compute the time since their most recent purchase ($r$), the number of visits or purchases made ($f$) and the amount spent per visit or the total amount spent ($m$). Next, we divide each category into five equal groups and assign a score (from 1 to 5) for each customer within that category. Higher scores correspond to more recent, more frequent and more valuable customers.\n",
    "\n",
    "Suppose that the original customers made the following transactions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../figs/how_does_rfm_work_1.png\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then after the division into groups, the RFM scores would be:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../figs/how_does_rfm_work_2.png\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, if appropriate, a business may divide up their history into periods, and study how customers' RFM scores have changed. In such a scenario, if a customer changes from 555 to 125, it may be an indication that a customer is about to churn. It would be worth a try to offer him or her a retention package. On the other hand, if a customer changes from 531 to 555, the customer has the potential to be a loyal one. It is worthwhile to send them a note or a discount voucher to reward them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdgrouped = cdnow2.groupby('cid')\n",
    "\n",
    "cd2 = cdgrouped.agg({'amnt': np.sum, 'cid': len, 'week_num': np.max})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd2.loc[:, 'r'] = cd2.week_num - cdnow2.week_num.max()\n",
    "cd2.rename(columns = {'amnt':'m', 'cid':'f'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd2.f.describe(percentiles=[.2, .4, .6,. 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.qcut(cd2.f, [0, 0.2, 0.4, 0.6, 0.8, 1.0], labels=[1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd2['R'] = pd.qcut(cd2.r, [0, 0.2, 0.4, 0.6, 0.8, 1.0], labels=[1, 2, 3, 4, 5])\n",
    "# cd2['F'] = pd.qcut(cd2.f, [0, 0.2, 0.4, 0.6, 0.8, 1.0], labels=[1, 2, 3, 4, 5])\n",
    "cd2['F'] = pd.qcut(cd2.f, [0, 0.2, 0.4, 0.6, 0.8, 1.0], labels=[1, 4, 5], duplicates='drop')\n",
    "cd2['M'] = pd.qcut(cd2.m, [0, 0.2, 0.4, 0.6, 0.8, 1.0], labels=[1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd2.loc[:, 'RFM'] = cd2.loc[:, ['R', 'F', 'M']].apply(\n",
    "    lambda x: str(x[0]) + str(x[1]) + str(x[2]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab([cd2.R, cd2.F], cd2.M, margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd2.pivot_table(index='F', columns='R', values='m', aggfunc='mean').round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using quintiles, let us use the raw features and perform hierarchical clustering - let the data speak for itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd3 = cd2[['r', 'f', 'm']].copy()\n",
    "cd3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='hclust'></a>\n",
    "### Hierarchical Clustering\n",
    "<a href=#top>(back to top)</a>\n",
    "\n",
    "Instead of converting the recency, frequency and monetary variables into quintiles and then studying them, we can use an **unsupervised learning** technique to discover the patterns in the data. In particular, we can perform cluster analysis to segment the data. Our goal here is to assign customers into groups, or clusters, such that those within each cluster are more \"closely related\" to one another than with customers in another cluster.\n",
    "\n",
    "For instance, here is what the output of a clustering algorithm would like when applied to the 2-variable data on the left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = default_rng(5003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating data (that I know comes from two groups)\n",
    "X1 = rng.normal(loc=2.0, scale=0.5, size=20).reshape((10,2))\n",
    "X2 = rng.normal(loc=-2.0, scale=0.5, size=20).reshape((10,2))\n",
    "dummy_data = np.vstack([X1, X2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting and identifying members of two clusters\n",
    "Z = hierarchy.linkage(dummy_data)\n",
    "grps = hierarchy.cut_tree(Z, 2).ravel()\n",
    "fc = np.where(grps == 1, 'red', 'blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualising the original data, and the identified clusters.\n",
    "fig, (ax1,ax2) = plt.subplots(1, 2, figsize=(12,4))\n",
    "ax1.scatter(dummy_data[:,0], dummy_data[:, 1], facecolor='black'); ax1.set_title('Before Clustering');\n",
    "ax2.scatter(dummy_data[:,0], dummy_data[:, 1], facecolor=fc); ax2.set_title('Clustering Algo Output');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many different clustering algorithms. You may have heard of K-means, a very popular one, before. The one we are going to use here is very similar to it. It is known as **agglomerative hierarchical clustering**. Let's take a look at how it works first.\n",
    "\n",
    "<img src=\"../figs/clust_agglomerative_hierarchical.png\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Does The Clustering Algorithm Work?\n",
    "\n",
    "#### Dissimilarity Measures Between Individual Observations\n",
    "\n",
    "As we mentioned earlier, cluster analysis tries to identify groups such that those within a group are \"similar\" to one another. In order to proceed, we need to formalise this idea of similarity/dissimilarity. \n",
    "\n",
    "Most of the time, we have $N$ observations $x_1, x_2, \\dots, x_N$ and we wish to group them into $K$ clusters. Each observation is typically a vector of $p$ observed values, so we may write $x_i = (x_{i1}, x_{i2}, \\dots, x_{ip})$ for instance. Referring back to our CD example, we had $N=23570$ customers, and we had engineered $p=3$ features: recency, frequency and monetary value.\n",
    "\n",
    "Most clustering algorithms require a dissimilarity matrix as input, so we need a function that can measure **pairwise dissimilarity**. One of the most common choices is the Euclidean distance (or rather the $L_2$-norm) between $x_i$ and $x_j$:\n",
    "\\begin{equation*}\n",
    "d(x_i,\\;x_j) = \\bigg\\{\\sum_{s=1}^p (x_{is} - x_{js})^2\\bigg\\}^{1/2}\n",
    "\\end{equation*}\n",
    "\n",
    "Another common choice is the $L_1$-norm:\n",
    "\\begin{equation*}\n",
    "d(x_i,\\;x_j) = \\sum_{s=1}^p |x_{is} - x_{js}|\n",
    "\\end{equation*}\n",
    "\n",
    "#### Dissimilarity Measures Between Clusters or Groups\n",
    "\n",
    "For hierarchical clustering, we need to build on this choice of pairwise dissimilarity to obtain a measure of dissimilarity between groups. In other words, suppose we have two groups of points $G$ and $H$, with $N_G$ and $N_H$ points within them respectively. We wish to use the pairwise dissimilarity between points in $G$ and $H$, to compute a dissimilarity between $G$ and $H$. We call this the **linkage method**, and there are several options for this too:\n",
    "\n",
    "1. Single linkage takes the intergroup dissimilarity to be that of the closest (least dissimilar) pair.\n",
    "\\begin{equation*}\n",
    "d_S(G,H) = \\min_{i \\in G, j \\in H} d(x_i,\\, x_j)\n",
    "\\end{equation*}\n",
    "2. Complete linkage takes the intergroup dissimilarity to be that of the furthest (most dissimilar) pair.\n",
    "\\begin{equation*}\n",
    "d_C(G,H) = \\max_{i \\in G, j \\in H} d(x_i,\\, x_j)\n",
    "\\end{equation*}\n",
    "3. Average linkage utilises the average of all pairwise dissimilarities between the groups:\n",
    "\\begin{equation*}\n",
    "d_A(G,H) = \\frac{1}{N_G N_H} \\sum_{i \\in G} \\sum_{j \\in H} d(x_i,x_j)\n",
    "\\end{equation*}\n",
    "4. Ward linkage uses a more complicated distance to minimise the variance within groups. It usually returns more compact clusters than the others. Suppose that group $G$ was formed by merging groups $G_1$ and $G_2$. Then the Ward distance between groups is \n",
    "\\begin{equation*}\n",
    "d_W(G,H) = \\left\\{\\frac{|H| + |G_1|}{N_G + N_H}d_W(H,G_1)^2 + \\frac{|H| + |G_2|}{N_G + N_H}d_W(H,G_2)^2 + \\frac{|H|}{N_G +N_H}d_W(G_1,G_2)^2\\right\\}^{1/2}\n",
    "\\end{equation*}\n",
    "\n",
    "The choice of linkage can affect the final clusters we obtain, so it is important to choose carefully based on the subject matter.\n",
    "\n",
    "<img src=\"../figs/clust_linkage_mtds.png\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agglomerative Hierarchical Clustering Algorithm\n",
    "\n",
    "The output of the algorithm is a hierarchical representation of the data, where clusters at each level of the \n",
    "hierarchy are created by merging clusters at the next lower level. At the lowest level, each cluster contains a single observation. At the highest level there is only one cluster containing all of the data.\n",
    "\n",
    "Starting at the bottom (with $N$ clusters of singletons), we recursively merge a selected pair of\n",
    "clusters into a single cluster. This produces a grouping at the next higher level with one less cluster. The pair chosen for merging consist of the two groups with the smallest intergroup dissimilarity. \n",
    "\n",
    "As you can tell, this algorithm does not require the number of clusters as an input. The final number of clusters can be based on a visualisation of this hierarchy of clusterings, through a dendrogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[.25, .7], [.3, .8], [.7, .6]])\n",
    "fc_dict={'Stage 0': ['red', 'blue', 'green'], 'Stage 1': ['red', 'red', 'green'], \n",
    "        'Stage 2': ['red']*3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist, squareform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdist(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squareform(pdist(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "for x,y in enumerate(fc_dict.items()):\n",
    "    plt.subplot(1,3,x+1)\n",
    "    plt.scatter(X[:,0], X[:,1], facecolor=y[1]);\n",
    "    plt.ylim(0.2,1); plt.xlim(0,1)\n",
    "    #plt.grid(); \n",
    "    plt.title(y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the number of clusters changed from 3 to 2 and then to 1. Here is how we can visualise the hierarchy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm0 = hierarchy.linkage(X)\n",
    "hierarchy.dendrogram(lm0, p=2)\n",
    "plt.title('A Dendrogram');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dendrogram shows that points with index 0 and 1 (the closest two points) merge at a small vertical distance (height of orange lines), but the group containing them merges with point 2 at a much higher vertical distance (blue line on the left). This shows that points 0 and 1 are less dissimilar to one another than they are (as a group) to point 2. In other words, the *height of each node is proportional to the value of the intergroup dissimilarity between its two child nodes*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back to CDNOW Customers\n",
    "\n",
    "Now that we know a technique for segmenting data, let us apply it to our customers. Before we do so, we shall scale the dataset so that each of the three columns R,F and M has 0 mean and variance 1. This prevents a particular column from dominating the results because it is orders of magnitude larger than the others. We shall carry out this scaling by subtracting the column mean and then dividing by the column standard deviation for each observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cd3.to_numpy()\n",
    "X_scaled = ((X - X.mean(axis=0))/X.std(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm1 = hierarchy.linkage(X_scaled, method='ward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "hierarchy.dendrogram(lm1, p=3, truncate_mode='level');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems reasonable that we can split the customers into three groups. Can we categorise these groups in some way?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = hierarchy.cut_tree(lm1, n_clusters=3).ravel()\n",
    "cd3['groups'] = out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cd3.groupby('groups').agg([np.median, lambda x: np.quantile(x, .75) - np.quantile(x, .25)])\n",
    "# cd3.groupby('groups').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd3.groups.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd3.boxplot('m', by=\"groups\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determining the optimal number of clusters\n",
    "\n",
    "We used 3 clusters, but can we come up with a more formal method of determining the optimal number of clusters?\n",
    "\n",
    "The Silhouette coefficient summarises the within similarity to the between similarity using the following formula:\n",
    "\n",
    "\\begin{equation*}\n",
    "S = \\frac{b-a}{\\max(a,b)}\n",
    "\\end{equation*}\n",
    "\n",
    "where $a$ is the average distance between an observation and all other observations in the same cluster and $b$ is the average distance between an observation and all other observations in the *next nearest cluster*. This coefficient takes values between -1 and 1, with values closer to 1 indicating a more optimal clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clust.compute_silhouette_scores(lm1, X_scaled, [2,3,4,5,6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='mba'></a>\n",
    "## Market Basket Analysis\n",
    "<a href=#top>(back to top)</a>\n",
    "\n",
    "Association rules analysis is a very popular tool for mining commercial databases. It is applicable when you have transaction-level data. In other words, the observations in your data consist of sales transactions - items purchased by customers at a physical or online store. The goal of an association rules analysis is to identify items that are frequently bought together. This information can be very useful in \n",
    "\n",
    "  * stocking shelves at the physical store, or even at the warehouse,\n",
    "  * cross-marketing in sales promotions, and \n",
    "  * designing catalogues for mail-in promotions or advertisements.\n",
    "  \n",
    "### Apriori Algorithm\n",
    "\n",
    "Customer databases can be very huge. It is computationally infeasible to compute the proportion of all $2^N$ types of items. The apriori algorithm gets around this using this clever trick:\n",
    "\n",
    "> If an itemset is infrequent, any itemset that contains it is also infrequent.\n",
    "\n",
    "For instance, if I know that `{caviar, eggs}` are bought very rarely, then `{caviar, eggs, wine}` will be even more rarely purchased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are Association Rules?\n",
    "\n",
    "Let $X$ and $Y$ be itemsets. They denote possible \"baskets\" of items that could be purchased together. Suppose that there have been $N$ transactions in a database. Then we define the support of an itemset to be \n",
    "\n",
    "\\begin{equation*}\n",
    "\\text{Support}(X) = \\frac{\\text{no. of transactions that contain $X$}}{N}\n",
    "\\end{equation*}\n",
    "\n",
    "The goal of this section is to identify *interesting* rules of the form:\n",
    "\\begin{equation*}\n",
    "X \\Rightarrow Y\n",
    "\\end{equation*}\n",
    "which we interpret as: if a customer purchases itemset $X$ (the *antecedent*), then he will purchase itemset $Y$ (the *consequent*). Rules are not certainties - that is why we need to compute several metrics on any postulated rule before we can act on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions\n",
    "\n",
    "1. The support of a rule $X \\Rightarrow Y$ is defined to be \n",
    "\n",
    "\\begin{equation*}\n",
    "\\text{Support}(X \\Rightarrow Y) = \\frac{\\text{no. of transactions that contain $X$ and $Y$}}{N} = P(X \\cup Y)\n",
    "\\end{equation*}\n",
    "\n",
    "2. The confidence of a rule $X \\Rightarrow Y$ is defined to be \n",
    "\n",
    "\\begin{equation*}\n",
    "\\text{Confidence}(X \\Rightarrow Y) = \\frac{\\text{no. of transactions that contain $X$ and $Y$}}{\\text{no. of transactions that contain $X$}} = P( Y | X)\n",
    "\\end{equation*}\n",
    "\n",
    "The confidence of a rule will be a value between 0 and 1. It gives us an indication of the chance that a customer will add on to his current purchase.\n",
    "\n",
    "3. The lift of a rule $X \\Rightarrow Y$ is defined to be \n",
    "\n",
    "\\begin{equation*}\n",
    "\\text{Lift}(X \\Rightarrow Y) = \\frac{\\text{Confidence}(X \\Rightarrow Y)}{\\text{Support}(Y)} = P( Y | X) / P(Y)\n",
    "\\end{equation*}\n",
    "\n",
    "Note that the lift of $X \\Rightarrow Y$ is the same as the lift of $Y \\Rightarrow X$. The lift of a rule is a value between 0 and $\\infty$. A value of 1 denotes that there is no value in promoting $Y$ to a customer who intends to purchase $X$. Values greater than 1 indicate a useful rule for cross-promotion.\n",
    "\n",
    "4. The leverage of a rule $X \\Rightarrow Y$ is defined to be \n",
    "\n",
    "\\begin{equation*}\n",
    "\\text{Leverage}(X \\Rightarrow Y) = \\text{Support}(X \\Rightarrow Y) - \\text{Support}(Y)\\text{Support}(X)\n",
    "\\end{equation*}\n",
    "\n",
    "This will be a value between -1 and 1. It gives an indication of association between the products. If leverage is positive, it means that customers were more likely to buy these two itemsets together, compared to the chance of buying them if these two itemsets were independent.\n",
    "\n",
    "5. The conviction of a rule $X \\Rightarrow Y$ is defined to be \n",
    "\n",
    "\\begin{equation*}\n",
    "\\text{Conviction}(X \\Rightarrow Y) = \\frac{1 - \\text{Support}(Y)}{1 - \\text{Confidence}(X \\Rightarrow Y)} =\n",
    "\\frac{P(Y^c)}{P(Y^c | X)}\n",
    "\\end{equation*}\n",
    "\n",
    "A value more than 1 indicates that $Y$ is more likely bought with $X$ than without. Conviction was conceived of to replace lift, which is non-directional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a simple example before applying these ideas to a larger dataset. Consider the following table of transactions:\n",
    "\n",
    "| transaction | itemset                 |\n",
    "|-------------|-------------------------|\n",
    "| 1           | butter, jelly           |\n",
    "| 2           | butter, jelly           |\n",
    "| 3           | p.butter, butter        |\n",
    "| 4           | p.butter, butter, jelly |\n",
    "| 5           | p.butter                |\n",
    "\n",
    "Now let us focus on a particular rule: $\\text{butter} \\Rightarrow \\text{jelly}$.\n",
    "\n",
    "1. The support of the rule is 3/5.\n",
    "2. The confidence of the rule is 3/4.\n",
    "3. The lift of the rule is .75/(3/5) = 1.25\n",
    "4. The leverage of the rule is .6 - .8(.6) = .12\n",
    "5. The conviction of the rule is .4/(1-.75) = 1.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset that we shall consider next consists of gifts purchased from an online retailer. The original dataset was obtained here: https://archive.ics.uci.edu/ml/datasets/Online+Retail+II, but the version we have here is heavily cleaned. Initially, there were 3843 different types of items. They were re-categorised, with those that occurred too often being dropped. The first column is an invoice number, indicating which items were bought together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trans2 = pd.read_csv('../data/online_gift_trans.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trans2.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trans2.desc2.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_list = all_trans2.groupby('InvoiceNo').apply(lambda x: x.desc2.to_list()).to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_list[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(trans_list).transform(trans_list, sparse=True)\n",
    "df2 = pd.DataFrame.sparse.from_spmatrix(te_ary, columns=te.columns_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input to the apriori algorithm requires a boolean matrix with one row for each transaction (invoice), and one column for each item. As we can see, we have 34 categories of items now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap_out = apriori(df2, min_support=0.01, use_colnames=True, max_len=4, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect the frequent itemsets since it is just a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ap_out.head()\n",
    "ap_out[ap_out.itemsets <= {'BUNTING', 'CLOCKS'}]\n",
    "#ap_out[ap_out['itemsets'] >= {'LIGHTS', 'TOYS'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap_out.sort_values('support', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we proceed to generate association rules with a minimum confidence of 0.5, and try to answer a few questions of interest:\n",
    "\n",
    "1. What is an interesting rule that I can use?\n",
    "2. I have an excess of notebooks - what can I pair them with to sell them off?\n",
    "3. A customer has tissues and towels in his shopping cart - what should I recommend to him?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar = association_rules(ap_out, metric='confidence', min_threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar[(ar.confidence > 0.7) & (ar.antecedents.apply(len) <= 2)].sort_values(by='lift', ascending =False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding question 1, it looks like I can place or stock SIGNS close to key chains, bathroom accessories and wallets since the lift, leverage and conviction are all high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar[(ar.consequents == {'NOTEBOOKS'}) & (ar.antecedents.apply(len) <= 2)].sort_values(by='conviction', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The support for key chains and stationery is the highest in the above table, and the conviction is high. So I might consider pairing notebooks with those items. I could also consider pairing it with magnets and baskets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar[ar.antecedents == {'TOWELS', 'TISSUES'}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I should recommend stationery to him."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some caveats when using Market Basket Analysis\n",
    "\n",
    "Market basket analysis is taught in many books, and there are many papers on the algorithms to derive the association rules from huge databases. However, you will find comparatively less papers and articles on the success from using Market Basket Analysis (an exception is the Target story).\n",
    "\n",
    "There could be a reason for this. Suppose that a store finds that two brands of soft drink (A and B) are often bought together. Hence, they decide to run a promotion on brand A, hoping that it would trigger an increase in the sales of brand B. Instead, the sales of brand B dropped dramatically!!?\n",
    "\n",
    "This happens when the two products have a substitute relationship instead of a complementary one. When customers see a price drop in one of them, they switch to it, at the expense of the one whose price did not drop.\n",
    "\n",
    "What can we do to avoid situations like this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ref'></a>\n",
    "### References and Summary\n",
    "<a href=#top>(back to top)</a>\n",
    "\n",
    "1. [CDNOW dataset download](https://www.brucehardie.com/datasets/) (also available on Canvas)\n",
    "2. [An Introduction to Statistical Learning](https://hastie.su.domains/ISLR2/ISLRv2_website.pdf) Chapter 12 of this book covers hierarchical clustering in more detail, along with other clustering techniques. \n",
    "3. *Drilling down: turning customer data into profits with a spreadsheet*, by Jim Novo. This book covers several methods in customer analytics, and shows you how to use them on a spreadsheet. It is available at our library."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
